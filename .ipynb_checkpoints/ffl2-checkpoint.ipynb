{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the injury probabilities from Sports Injury Predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a list of the URLs we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All the URLs are the same except for the position, so we can create a list of strings for that.\n",
    "pos = ['qb','rb','wr','te']\n",
    "\n",
    "# Use list comprehension to iterate through that list and complete the URL, making a final list of URLs to use.\n",
    "url_list = ['http://sportsinjurypredictor.com/injury-predictor/injury-predictor/search-by-position/'+string+'/low-to-high' for string in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a big data frame with all the players' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an empty data frame that has the same column label as what we'll be appending later.\n",
    "df_injury = pd.DataFrame(columns=['Name','Injury'])\n",
    "\n",
    "# Start the for loop over list of URLs (websites for different positions).\n",
    "for url in url_list:\n",
    "\n",
    "    # Download the page.\n",
    "    req = requests.get(url)\n",
    "\n",
    "    # Assign the text of the page to a variable.\n",
    "    page = req.text\n",
    "\n",
    "    # Parse the HTML code.\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    # Download the rows.\n",
    "    rows = [row for row in soup.find(\"div\", \"page\").find_all(\"div\",\"player-card margin-top-20\")]\n",
    "    \n",
    "    # Get the row names\n",
    "    names_list = [row.find(\"span\",\"h3 face-myers c005b99\").get_text() for row in rows]\n",
    "\n",
    "    # Make a data frame for that.\n",
    "    df_names_list = pd.DataFrame(names_list, columns=['Name'])\n",
    "    \n",
    "\n",
    "    # Get rid of all the spaces and new lines and the % sign. We just want the number.\n",
    "    # The argument for strip is all the characters I want to get rid of.\n",
    "    percentages_list = [row.find('div','h1 face-myers inline c4a9f00').get_text().strip('\\t\\n%') for row in rows]\n",
    "    \n",
    "    # Those number are strings. If you want to do any math with them later, make them numbers, using \"float\".\n",
    "    # We also want to divide by 100 to get a percentage.\n",
    "    percentages_list = [float(i)/100 for i in percentages_list]\n",
    "    # Make a data frame for that.\n",
    "    df_percentages_list = pd.DataFrame(percentages_list, columns=['Injury'])\n",
    "    \n",
    "    # Create a data frame in Pandas by concatenating those data frames.\n",
    "    df_this_position = pd.concat([df_names_list, df_percentages_list], axis=1)\n",
    "\n",
    "    # We want each data frame to be named by position. \n",
    "    # We'll do this by finding out where in the url_list loop we are, then using that as an index for the pos list.\n",
    "    current_position = pos[url_list.index(url)]\n",
    "    # This next part is somewhat inelegant. Next time find a way to create strings within the variable name.\n",
    "    if current_position == 'qb':\n",
    "        df_inj_qb = df_this_position\n",
    "    elif current_position == 'rb':\n",
    "        df_inj_rb = df_this_position\n",
    "    elif current_position == 'wr':\n",
    "        df_inj_wr = df_this_position\n",
    "    elif current_position == 'te':\n",
    "        df_inj_te = df_this_position\n",
    "    \n",
    "    # Append that to the big data frame with the rankings data for all players.\n",
    "    df_injury = df_injury.append(df_this_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary for reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_injury = df_injury.set_index('Name').to_dict()\n",
    "\n",
    "# I don't fully understand to_dict, and it includes the column name as the first dictionary entry...\n",
    "# ...so to access Cam Newton you'd have to type \"dict_injury['Injury']['Cam Newton']. To simplify it I'm doing this:\n",
    "dict_injury = dict_injury['Injury']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the projections from Fantasy Pros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the URLs are the same except for the position, so we can create a list of strings for that.\n",
    "pos = ['qb','rb','wr','te']\n",
    "\n",
    "# Use list comprehension to iterate through that list and complete the URL, making a final list of URLs to use.\n",
    "url_list = ['https://www.fantasypros.com/nfl/projections/'+string+'.php' for string in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I want to keep the positions separate, so we won't be making one large dataframe like we did for injuries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________________FOR NON-PPR SCORING:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Start the for loop over list of URLs (websites for different positions).\n",
    "for url in url_list:\n",
    "\n",
    "    # Download the page.\n",
    "    req = requests.get(url)\n",
    "\n",
    "    # Assign the text of the page to a variable.\n",
    "    page = req.text\n",
    "\n",
    "    # Parse the HTML code.\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    # Download the rows from the table by finding all the \"tr\" tags (rows) with the class \"mpb-available\".\n",
    "    # I knew they needed that particular class because I looked at the tags in Chrome Developer Tools on the website.\n",
    "    rows = [row for row in soup.find('table', id='data').find_all('tr')[2:]]\n",
    "    # This used to be # rows = [row for row in soup.find('table', id='data').find_all('tr', 'mpb-available')]\n",
    "    # ... but the website changed its format so mpb-available wasn't the class anymore...\n",
    "    # ... each class had a different name so I had to take all the tr's and get rid of the first two (headers).\n",
    "\n",
    "    # Get the row names by looking at the text in the \"a\" tag for each row.\n",
    "    names_list = [row.find('a').get_text() for row in rows]\n",
    "    # Make a data frame for that.\n",
    "    df_names_list = pd.DataFrame(names_list, columns=['Name'])\n",
    "\n",
    "    # All we're interested in is the projected points for each player.\n",
    "    # That's the last \"td\" tag in each row. Use Beautiful Soup's \"find_all\" with a slice for just the last element.\n",
    "    # That's different for each position, so we need to calculate the length of the list of td's and find the last.\n",
    "    last_element_index = len(rows[0].find_all('td'))-1 # subtract 1 to get the correct index\n",
    "    projections_list = [row.find_all('td')[last_element_index].get_text() for row in rows]\n",
    "    # Those number are strings. If you want to do any math with them later, make them numbers, using \"float\".\n",
    "    projections_list = [float(i) for i in projections_list]\n",
    "    # Make a data frame for that.\n",
    "    df_projections_list = pd.DataFrame(projections_list, columns=['Projection'])\n",
    "    \n",
    "    # Create a data frame in Pandas by concatenating those data frames.\n",
    "    df_this_position = pd.concat([df_names_list, df_projections_list], axis=1)\n",
    "\n",
    "    # We want each data frame to be named by position. \n",
    "    # We'll do this by finding out where in the url_list loop we are, then using that as an index for the pos list.\n",
    "    current_position = pos[url_list.index(url)]\n",
    "    # This next part is somewhat inelegant. Next time find a way to create strings within the variable name.\n",
    "    if current_position == 'qb':\n",
    "        df_proj_qb = df_this_position\n",
    "    elif current_position == 'rb':\n",
    "        df_proj_rb = df_this_position\n",
    "    elif current_position == 'wr':\n",
    "        df_proj_wr = df_this_position\n",
    "    elif current_position == 'te':\n",
    "        df_proj_te = df_this_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___________________FOR HALF-PPR SCORING:\n",
    "\n",
    "### For RBs and WRs, we have to do some math with the raw projection data because Fantasy Pros does its calculations for regular scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start the for loop over list of URLs (websites for different positions).\n",
    "for url in url_list:\n",
    "\n",
    "    # Download the page.\n",
    "    req = requests.get(url)\n",
    "\n",
    "    # Assign the text of the page to a variable.\n",
    "    page = req.text\n",
    "\n",
    "    # Parse the HTML code.\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    # Download the rows from the table by finding all the \"tr\" tags (rows) with the class \"mpb-available\".\n",
    "    # I knew they needed that particular class because I looked at the tags in Chrome Developer Tools on the website.\n",
    "    rows = [row for row in soup.find('table', id='data').find_all('tr')[2:]]\n",
    "    # This used to be # rows = [row for row in soup.find('table', id='data').find_all('tr', 'mpb-available')]\n",
    "    # ... but the website changed its format so mpb-available wasn't the class anymore...\n",
    "    # ... each class had a different name so I had to take all the tr's and get rid of the first two (headers).\n",
    "\n",
    "    # Get the row names by looking at the text in the \"a\" tag for each row.\n",
    "    names_list = [row.find('a').get_text() for row in rows]\n",
    "    # Make a data frame for that.\n",
    "    df_names_list = pd.DataFrame(names_list, columns=['Name'])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # QB scoring is unaffected by PPR scoring, but the other positions will require some math. \n",
    "    # Get the position by finding out where in the url_list loop we are, then using that as an index for the pos list.\n",
    "    current_position = pos[url_list.index(url)]\n",
    "    \n",
    "    if current_position == 'qb':\n",
    "    \n",
    "        # Use the last \"td\" tag in each row. Use Beautiful Soup's \"find_all\" with a slice for just the last element.\n",
    "        # That's different for each position, so we need to calculate the length of the list of td's and find the last.\n",
    "        last_element_index = len(rows[0].find_all('td'))-1 # subtract 1 to get the correct index\n",
    "        projections_list = [row.find_all('td')[last_element_index].get_text() for row in rows]\n",
    "        # Those number are strings. If you want to do any math with them later, make them numbers, using \"float\".\n",
    "        projections_list = [float(i) for i in projections_list]\n",
    "\n",
    "    \n",
    "    # We also need to do this differently for each position because the rows of data have different stats.\n",
    "    # e.g. The TE page on Fantasy Pros doesn't list projected carries, that's only for RBs.\n",
    "    # Conveniently, the RB and WR pages have the same stats listed in the same order.\n",
    "    # We're also going to convert directly to float and calculate points, so this is 3 nested loops in each line. Woah!\n",
    "    # We need to get rid of commas because the float command can't handle that. This comes up when they get 1,000 yards.\n",
    "    \n",
    "    elif current_position == 'rb' or current_position == 'wr':\n",
    "  \n",
    "        rushyards = [x/10 for x in   [float(i) for i in [row.find_all('td')[2].get_text().replace(',','') for row in rows]]]\n",
    "        rushtds =   [x*6 for x in    [float(i) for i in [row.find_all('td')[3].get_text() for row in rows]]]\n",
    "        rec =       [x/2 for x in    [float(i) for i in [row.find_all('td')[4].get_text() for row in rows]]]\n",
    "        recyards =  [x/10 for x in   [float(i) for i in [row.find_all('td')[5].get_text().replace(',','') for row in rows]]]\n",
    "        rectds =    [x*6 for x in    [float(i) for i in [row.find_all('td')[6].get_text() for row in rows]]]\n",
    "        fumbles =   [x*(-2) for x in [float(i) for i in [row.find_all('td')[7].get_text() for row in rows]]]\n",
    "        \n",
    "        projections_list = [rushyards[i]+rushtds[i]+rec[i]+recyards[i]+rectds[i]+fumbles[i] for i in range(len(rec))]\n",
    "        \n",
    "        \n",
    "    elif current_position == 'te':\n",
    "  \n",
    "        rec =       [x/2 for x in    [float(i) for i in [row.find_all('td')[1].get_text() for row in rows]]]\n",
    "        recyards =  [x/10 for x in   [float(i) for i in [row.find_all('td')[2].get_text().replace(',','') for row in rows]]]\n",
    "        rectds =    [x*6 for x in    [float(i) for i in [row.find_all('td')[3].get_text() for row in rows]]]\n",
    "        fumbles =   [x*(-2) for x in [float(i) for i in [row.find_all('td')[4].get_text() for row in rows]]]\n",
    "        \n",
    "        projections_list = [rec[i]+recyards[i]+rectds[i]+fumbles[i] for i in range(len(rec))]\n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Make a data frame for that.\n",
    "    df_projections_list = pd.DataFrame(projections_list, columns=['Projection'])     \n",
    "    \n",
    "    # Create a data frame in Pandas by concatenating those data frames.\n",
    "    df_this_position = pd.concat([df_names_list, df_projections_list], axis=1)\n",
    "\n",
    "    # This next part is somewhat inelegant. Next time find a way to create strings within the variable name.\n",
    "    if current_position == 'qb':\n",
    "        df_proj_qb = df_this_position\n",
    "    elif current_position == 'rb':\n",
    "        df_proj_rb = df_this_position\n",
    "    elif current_position == 'wr':\n",
    "        df_proj_wr = df_this_position\n",
    "    elif current_position == 'te':\n",
    "        df_proj_te = df_this_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match the injury data and the projection data to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the boring errors where the player names don't match or they aren't listed in both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you get an error when running this program it might be because the player names don't match.\n",
    "# For example, \"Odell Beckham\" vs \"Odell Beckham Jr.\"\n",
    "# Fix it case by case here.\n",
    "\n",
    "# This changes \"Odell Beckham\" in the injury dictionary to \"Odell Beckham Jr.\" to match the Fantasy Pros data.\n",
    "dict_injury['Odell Beckham Jr.'] = dict_injury.pop('Odell Beckham')\n",
    "dict_injury['Robert Griffin'] = dict_injury.pop('Robert Griffin III')\n",
    "\n",
    "# For some reason, many players don't show up in the Sports Injury Predictor website...\n",
    "# ... so I'm going to make their injury probability the average of the other players at their position.\n",
    "\n",
    "# Calculate the averages.\n",
    "avg_qb = float(df_inj_qb.mean())\n",
    "avg_rb = float(df_inj_rb.mean())\n",
    "avg_wr = float(df_inj_wr.mean())\n",
    "avg_te = float(df_inj_te.mean())\n",
    "\n",
    "# QBs\n",
    "dict_injury['Ryan Fitzpatrick'] = avg_qb\n",
    "dict_injury['Shaun Hill'] = avg_qb\n",
    "dict_injury['Trevor Siemian'] = avg_qb\n",
    "dict_injury['Case Keenum'] = avg_qb\n",
    "\n",
    "# RBs\n",
    "dict_injury['Jeremy Langford'] = avg_rb\n",
    "dict_injury['Dion Lewis'] = avg_rb\n",
    "dict_injury['Darren Sproles'] = avg_rb\n",
    "dict_injury['Javorius Allen'] = avg_rb\n",
    "dict_injury['DeAndre Washington'] = avg_rb\n",
    "dict_injury['Spencer Ware'] = avg_rb\n",
    "dict_injury['Chris Thompson'] = avg_rb\n",
    "dict_injury['Shaun Draughn'] = avg_rb\n",
    "dict_injury['Dan Herron'] = avg_rb\n",
    "dict_injury['Tim Hightower'] = avg_rb\n",
    "dict_injury['Chris Johnson'] = avg_rb\n",
    "dict_injury['Benny Cunningham'] = avg_rb\n",
    "dict_injury['Cameron Artis-Payne'] = avg_rb\n",
    "dict_injury['Paul Perkins'] = avg_rb\n",
    "dict_injury['Charcandrick West'] = avg_rb\n",
    "dict_injury['Khiry Robinson'] = avg_rb\n",
    "dict_injury['Robert Kelley'] = avg_rb\n",
    "dict_injury['Josh Ferguson'] = avg_rb\n",
    "dict_injury['Tyler Ervin'] = avg_rb\n",
    "dict_injury['Mike Gillislee'] = avg_rb\n",
    "dict_injury['Wendell Smallwood'] = avg_rb\n",
    "\n",
    "\n",
    "# WRs\n",
    "dict_injury['John Brown'] = avg_wr\n",
    "dict_injury['Devante Parker'] = avg_wr\n",
    "dict_injury['Stefon Diggs'] = avg_wr\n",
    "dict_injury['Tyler Lockett'] = avg_wr\n",
    "dict_injury['Willie Snead'] = avg_wr\n",
    "dict_injury['Josh Gordon'] = avg_wr\n",
    "dict_injury['Kamar Aiken'] = avg_wr\n",
    "dict_injury['Sammie Coates'] = avg_wr\n",
    "dict_injury['Phillip Dorsett'] = avg_wr\n",
    "dict_injury['Jermaine Kearse'] = avg_wr\n",
    "dict_injury['Ted Ginn'] = avg_wr\n",
    "dict_injury['Rishard Matthews'] = avg_wr\n",
    "dict_injury['Tajae Sharpe'] = avg_wr\n",
    "dict_injury['Davante Adams'] = avg_wr\n",
    "dict_injury['Chris Hogan'] = avg_wr\n",
    "dict_injury['Cole Beasley'] = avg_wr\n",
    "dict_injury['Seth Roberts'] = avg_wr\n",
    "\n",
    "\n",
    "# TEs\n",
    "dict_injury['Coby Fleener'] = avg_te\n",
    "dict_injury['Gary Barnidge'] = avg_te\n",
    "dict_injury['Zach Miller'] = avg_te\n",
    "dict_injury['Clive Walford'] = avg_te\n",
    "dict_injury['Will Tye'] = avg_te\n",
    "dict_injury['Crockett Gillmore'] = avg_te\n",
    "dict_injury['Vance McDonald'] = avg_te\n",
    "dict_injury['Richard Rodgers'] = avg_te\n",
    "dict_injury['Cameron Brate'] = avg_te\n",
    "dict_injury['Virgil Green'] = avg_te\n",
    "dict_injury['Jesse James'] = avg_te\n",
    "dict_injury['Lance Kendricks'] = avg_te\n",
    "dict_injury['Garrett Celek'] = avg_te\n",
    "dict_injury['Ryan Griffin'] = avg_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I don't know how to do this for each variable name so I'm going to repeat each line of code 4 times, one for each position. It's inelegant but it's simple and it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We only care about the top 10 QBs for a 10-team league, so get rid of the rest. \n",
    "# We also only want the top 20 RBs, 30 WRs, and 10 TEs.\n",
    "# In addition to getting rid of irrelavent data, this will make it easier to match the two data sets...\n",
    "# ...because they might not have the same number of players. Some might list every QB in the league, even free agents.\n",
    "# However, I'm going to go a little farther because the 10th best QB in projections might not actually be...\n",
    "# ...the 11th best QB if he has a high enough chance of injury.\n",
    "\n",
    "# For all players:\n",
    "N_qb = 12*3\n",
    "N_rb = 80\n",
    "N_wr = 80\n",
    "N_te = 12*3\n",
    "\n",
    "# Make a column of injury data that matches the names of the players in the projection data frame.\n",
    "injcolumn_qb = [dict_injury[df_proj_qb['Name'][player]] for player in range(0,N_qb)]\n",
    "injcolumn_rb = [dict_injury[df_proj_rb['Name'][player]] for player in range(0,N_rb)]\n",
    "injcolumn_wr = [dict_injury[df_proj_wr['Name'][player]] for player in range(0,N_wr)]\n",
    "injcolumn_te = [dict_injury[df_proj_te['Name'][player]] for player in range(0,N_te)]\n",
    "\n",
    "# Make a data frame for that.\n",
    "df_injcolumn_qb = pd.DataFrame(injcolumn_qb, columns=['Injury'])\n",
    "df_injcolumn_rb = pd.DataFrame(injcolumn_rb, columns=['Injury'])\n",
    "df_injcolumn_wr = pd.DataFrame(injcolumn_wr, columns=['Injury'])\n",
    "df_injcolumn_te = pd.DataFrame(injcolumn_te, columns=['Injury'])\n",
    "\n",
    "# Create a data frame in Pandas by concatenating that with the projection data frames.\n",
    "df_qb = pd.concat([df_proj_qb[0:N_qb], df_injcolumn_qb], axis=1)\n",
    "df_rb = pd.concat([df_proj_rb[0:N_rb], df_injcolumn_rb], axis=1)\n",
    "df_wr = pd.concat([df_proj_wr[0:N_wr], df_injcolumn_wr], axis=1)\n",
    "df_te = pd.concat([df_proj_te[0:N_te], df_injcolumn_te], axis=1)\n",
    "\n",
    "# Export to a CSV spreadsheet.\n",
    "df_qb.to_csv('FFL Injuries - QB.csv')\n",
    "df_rb.to_csv('FFL Injuries - RB.csv')\n",
    "df_wr.to_csv('FFL Injuries - WR.csv')\n",
    "df_te.to_csv('FFL Injuries - TE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average score for a backup of each position. This will be used in the spreadsheet formula to calculate how many points you get when the player is injured. \n",
    "\n",
    "It doesn't make sense to assume you'll get zero points if your player gets injured for a month. You'll have a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Avg Backup QB:  257\n",
      "\n",
      " Avg Backup RB:  142\n",
      "\n",
      " Avg Backup WR:  161\n",
      "\n",
      " Avg Backup TE:  123\n"
     ]
    }
   ],
   "source": [
    "avgbackup_qb = float(df_proj_qb[8:16].mean())\n",
    "avgbackup_rb = float(df_proj_rb[20:40].mean())\n",
    "avgbackup_wr = float(df_proj_wr[20:40].mean())\n",
    "avgbackup_te = float(df_proj_te[8:16].mean())\n",
    "\n",
    "print('\\n', 'Avg Backup QB: ', int(avgbackup_qb))\n",
    "print('\\n', 'Avg Backup RB: ', int(avgbackup_rb))\n",
    "print('\\n', 'Avg Backup WR: ', int(avgbackup_wr))\n",
    "print('\\n', 'Avg Backup TE: ', int(avgbackup_te))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
